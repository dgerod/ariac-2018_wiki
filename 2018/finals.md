# ARIAC 2018 Finals

Congratulations to the teams that successfully qualified for the competition through the Qualifiers.

For the ARIAC Finals, qualified teams will submit their system so that it can be run against previously-unseen scenarios.
Teams have already been exposed to all of the challenges that will be present in the Finals scenarios.
A single system must be designed that is agile enough to react to any combination of agility challenges, without knowing which will occur and when.

## What to expect

[The Finals specifications page](https://bitbucket.org/osrf/ariac/wiki/2018/finals_specs) has details on how the competition will be evaluated and what can be expected in the unseen competition scenarios.

_Update May 21 2018: [this page details the scenarios used in the ARIAC 2018 Finals](https://bitbucket.org/osrf/ariac/wiki/2018/finals_scenarios)._

## Preparing your system

To ensure that your system can adapt to previously-unseen scenarios, teams should test your system against all released sample trials and the trial configurations used for the Qualifiers.
[The competition configuration file specifications page](https://bitbucket.org/osrf/ariac/wiki/2018/configuration_spec) has details on how teams can create custom trial configuration files to test their system against additional scenarios.

## Submission process

Each team's system will be evaluated automatically against 15 scenarios.
The submission process will be the same as during the qualifiers, as outlined on [the automated evaluation page](https://bitbucket.org/osrf/ariac/wiki/2018/automated_evaluation).
Teams must submit new submission files for the Finals in order to be considered as a participant in the finals.
This is the case even if there were no changes since a team's qualifier submission.
Submissions will be uploaded via the private workspaces, as in the qualifiers.


Teams are also invited to submit an optional one-page synopsis of their approach and its value as an agile robotic system.
This synopsis will be provided to the human judges as part of their review of systems for judging.
[The challenge.gov page](https://www.challenge.gov/challenge/ariac/) summarizes the way in which automated scoring metrics will be combined with the judges' evaluations.

## Pre-Finals dry-run testing of submissions

Teams will be able to submit their system for testing on the machine used to run the competition for a limited amount of time before the Finals.
Teams' systems will be validated against sample trials, not the trials that will be used in the Finals.
The ARIAC competition team will work with teams to resolve problems related to running their system on that machine.

## Schedule

- Until May 6: Teams test their submission using the mock competition setup on their machine.
- May 6, 2018: Deadline for qualified teams to submit their entries for pre-competition dry-run testing (see above).
- May 7-11, 2018: Dry-run testing and iteration with teams to help resolve issues with their entries.
- May 11, 2018 @ 11:59pm PST: Deadline for final submissions from qualified teams, including the optional one-page system synopsis.
- May 12-17, 2018: Perform official competition runs.
- May 18, 2018: Announcement of competition results.